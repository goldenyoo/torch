{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2022/08/31"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/goldenyoo/miniforge3/envs/mac_cpu/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.utils.prune as prune\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "from scipy import io\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MAT file에서 data 읽고 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat_file = io.loadmat('/Users/goldenyoo/Library/Mobile Documents/com~apple~CloudDocs/BioCAS_prepare/Python_code/Data_center/one_dx/Calib_data_1.mat')\n",
    "\n",
    "K1 = mat_file['K1']\n",
    "A1 = mat_file['A1']\n",
    "\n",
    "K2 = mat_file['K2']\n",
    "A2 = mat_file['A2']\n",
    "\n",
    "Y1 = mat_file['Y1']\n",
    "Y2 = mat_file['Y2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total_data: 828 / input_size: 22 / Seq_len: 16\n",
      "\n",
      "k1 size:  torch.Size([828, 22, 16])\n",
      "k2 size:  torch.Size([828, 22, 16])\n",
      "a1 size:  torch.Size([828, 22, 16])\n",
      "a2 size:  torch.Size([828, 22, 16])\n",
      "\n",
      "y1 size: torch.Size([828, 1])\n",
      "y2 size: torch.Size([828, 1])\n"
     ]
    }
   ],
   "source": [
    "# K 특성에 대한 Class1 vs Class2 Data 가져오기\n",
    "k1 = torch.FloatTensor(K1)\n",
    "k1 = k1.transpose(0,2)\n",
    "\n",
    "k2 = torch.FloatTensor(K2)\n",
    "k2 = k2.transpose(0,2)\n",
    "\n",
    "# A 특성에 대한 Class1 vs Class2 Data 가져오기\n",
    "a1 = torch.FloatTensor(A1)\n",
    "a1 = a1.transpose(0,2)\n",
    "\n",
    "a2 = torch.FloatTensor(A2)\n",
    "a2 = a2.transpose(0,2)\n",
    "\n",
    "print( \"Total_data: {}\".format(k1.size()[0]), \"/ input_size: {}\".format(k1.size()[1]),\"/ Seq_len: {}\\n\".format(k1.size()[2]))\n",
    "print(\"k1 size: \",k1.size())\n",
    "print(\"k2 size: \",k2.size())\n",
    "print(\"a1 size: \",a1.size())\n",
    "print(\"a2 size: \",a2.size())\n",
    "\n",
    "# Y에 대한 Class1 vs Class2 Data 가져오기\n",
    "y1 = torch.LongTensor(Y1)\n",
    "y2 = torch.LongTensor(Y2)\n",
    "\n",
    "print(\"\\ny1 size:\",y1.size())\n",
    "print(\"y2 size:\",y2.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train data 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k_train size: torch.Size([1656, 22, 16])\n",
      "a_train size: torch.Size([1656, 22, 16])\n",
      "y_train size: torch.Size([1656, 1])\n"
     ]
    }
   ],
   "source": [
    "k_train = torch.cat([k1,k2],dim=0)\n",
    "a_train = torch.cat([a1,a2],dim=0)\n",
    "\n",
    "y_train = torch.cat([y1,y2],dim=0)\n",
    "print(\"k_train size: {}\".format(k_train.size()))\n",
    "print(\"a_train size: {}\".format(a_train.size()))\n",
    "print(\"y_train size: {}\".format(y_train.size()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### y_train의 one-hot coding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1656, 1])\n"
     ]
    }
   ],
   "source": [
    "# y_train_one_hot = F.one_hot(y_train-1,num_classes=2)\n",
    "# print(y_train_one_hot.size())\n",
    "# y_train_one_hot.squeeze_()\n",
    "# print(y_train_one_hot.size())\n",
    "\n",
    "y_train = y_train-1 # y를 0~1의 정수로 만들어야함.\n",
    "print(y_train.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset & DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "dataset = TensorDataset(k_train,a_train,y_train) # 각 tensor의 첫번째 dim이 일치해야한다\n",
    "\n",
    "# Data Split\n",
    "dataset_size = len(dataset)\n",
    "train_size = int(dataset_size * 0.8)\n",
    "validation_size = int(dataset_size * 0.1)\n",
    "test_size = dataset_size - train_size - validation_size\n",
    "\n",
    "train_dataset, valid_dataset, test_dataset = random_split(dataset, [train_size, validation_size, test_size])\n",
    "\n",
    "train_dataloader    = DataLoader(train_dataset  ,batch_size=      batch_size  , shuffle=True, drop_last=True)\n",
    "valid_dataloader    = DataLoader(valid_dataset  ,batch_size= int(batch_size/8), shuffle=True, drop_last=True)\n",
    "test_dataloader     = DataLoader(test_dataset   ,batch_size= int(batch_size/8), shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 3\n",
    "lstm_output_size = hidden_size\n",
    "input_size = 22\n",
    "n_class = 2\n",
    "\n",
    "dtype = torch.float\n",
    "\n",
    "class TextLSTM(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(TextLSTM, self).__init__()\n",
    "\n",
    "    self.lstm_1 = nn.LSTM(input_size=input_size, hidden_size=hidden_size, dropout=0.3)\n",
    "    self.lstm_2 = nn.LSTM(input_size=input_size, hidden_size=hidden_size, dropout=0.3)\n",
    "    self.fc = nn.Linear(hidden_size*2, n_class)\n",
    "\n",
    "  def forward(self, hidden_and_cell_k, hidden_and_cell_a, K_and_A):\n",
    "    (k, a) = K_and_A\n",
    "\n",
    "\n",
    "\n",
    "    k = k.transpose(1,2)\n",
    "    k = k.transpose(0,1)\n",
    "    a = a.transpose(1,2)\n",
    "    a = a.transpose(0,1)\n",
    "\n",
    "    outputs1, (h_n1,c_n1) = self.lstm_1(k, hidden_and_cell_k)\n",
    "    outputs2, (h_n2,c_n2) = self.lstm_2(a, hidden_and_cell_a)\n",
    "\n",
    "    outputs = torch.cat((h_n1[-1],h_n2[-1]), dim=1)  \n",
    "\n",
    "    model = self.fc(outputs)  # 최종 예측 최종 출력 층\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TextLSTM(\n",
      "  (lstm_1): LSTM(22, 3, dropout=0.3)\n",
      "  (lstm_2): LSTM(22, 3, dropout=0.3)\n",
      "  (fc): Linear(in_features=6, out_features=2, bias=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/goldenyoo/miniforge3/envs/mac_cpu/lib/python3.8/site-packages/torch/nn/modules/rnn.py:62: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    }
   ],
   "source": [
    "model = TextLSTM()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   0/300 Batch: 0 Cost: 0.697953\n",
      "Epoch   0/300 Batch: 2 Cost: 0.690622\n",
      "Epoch   0/300 Batch: 4 Cost: 0.691383\n",
      "Epoch   0/300 Batch: 6 Cost: 0.701679\n",
      "Epoch   0/300 Batch: 8 Cost: 0.680758\n",
      "Epoch   0/300 Batch: 10 Cost: 0.684780\n",
      "Epoch   0/300 Batch: 12 Cost: 0.689983\n",
      "Epoch   0/300 Batch: 14 Cost: 0.704870\n",
      "Epoch   0/300 Batch: 16 Cost: 0.685257\n",
      "Epoch   0/300 Batch: 18 Cost: 0.708204\n",
      "Epoch  50/300 Batch: 0 Cost: 0.015741\n",
      "Epoch  50/300 Batch: 2 Cost: 0.010727\n",
      "Epoch  50/300 Batch: 4 Cost: 0.010818\n",
      "Epoch  50/300 Batch: 6 Cost: 0.009802\n",
      "Epoch  50/300 Batch: 8 Cost: 0.011233\n",
      "Epoch  50/300 Batch: 10 Cost: 0.011984\n",
      "Epoch  50/300 Batch: 12 Cost: 0.028135\n",
      "Epoch  50/300 Batch: 14 Cost: 0.010016\n",
      "Epoch  50/300 Batch: 16 Cost: 0.008640\n",
      "Epoch  50/300 Batch: 18 Cost: 0.009658\n",
      "Epoch 100/300 Batch: 0 Cost: 0.005404\n",
      "Epoch 100/300 Batch: 2 Cost: 0.004853\n",
      "Epoch 100/300 Batch: 4 Cost: 0.012003\n",
      "Epoch 100/300 Batch: 6 Cost: 0.006435\n",
      "Epoch 100/300 Batch: 8 Cost: 0.005610\n",
      "Epoch 100/300 Batch: 10 Cost: 0.003546\n",
      "Epoch 100/300 Batch: 12 Cost: 0.003573\n",
      "Epoch 100/300 Batch: 14 Cost: 0.005138\n",
      "Epoch 100/300 Batch: 16 Cost: 0.005509\n",
      "Epoch 100/300 Batch: 18 Cost: 0.005732\n",
      "Epoch 150/300 Batch: 0 Cost: 0.002851\n",
      "Epoch 150/300 Batch: 2 Cost: 0.001536\n",
      "Epoch 150/300 Batch: 4 Cost: 0.001185\n",
      "Epoch 150/300 Batch: 6 Cost: 0.001067\n",
      "Epoch 150/300 Batch: 8 Cost: 0.001441\n",
      "Epoch 150/300 Batch: 10 Cost: 0.000740\n",
      "Epoch 150/300 Batch: 12 Cost: 0.001656\n",
      "Epoch 150/300 Batch: 14 Cost: 0.001046\n",
      "Epoch 150/300 Batch: 16 Cost: 0.000584\n",
      "Epoch 150/300 Batch: 18 Cost: 0.002311\n",
      "Epoch 200/300 Batch: 0 Cost: 0.000369\n",
      "Epoch 200/300 Batch: 2 Cost: 0.000880\n",
      "Epoch 200/300 Batch: 4 Cost: 0.000877\n",
      "Epoch 200/300 Batch: 6 Cost: 0.000728\n",
      "Epoch 200/300 Batch: 8 Cost: 0.000737\n",
      "Epoch 200/300 Batch: 10 Cost: 0.000640\n",
      "Epoch 200/300 Batch: 12 Cost: 0.000842\n",
      "Epoch 200/300 Batch: 14 Cost: 0.001023\n",
      "Epoch 200/300 Batch: 16 Cost: 0.000708\n",
      "Epoch 200/300 Batch: 18 Cost: 0.000356\n",
      "Epoch 250/300 Batch: 0 Cost: 0.000580\n",
      "Epoch 250/300 Batch: 2 Cost: 0.000330\n",
      "Epoch 250/300 Batch: 4 Cost: 0.000496\n",
      "Epoch 250/300 Batch: 6 Cost: 0.000476\n",
      "Epoch 250/300 Batch: 8 Cost: 0.000319\n",
      "Epoch 250/300 Batch: 10 Cost: 0.000394\n",
      "Epoch 250/300 Batch: 12 Cost: 0.000609\n",
      "Epoch 250/300 Batch: 14 Cost: 0.000434\n",
      "Epoch 250/300 Batch: 16 Cost: 0.000418\n",
      "Epoch 250/300 Batch: 18 Cost: 0.000549\n",
      "Epoch 300/300 Batch: 0 Cost: 0.000092\n",
      "Epoch 300/300 Batch: 2 Cost: 0.000212\n",
      "Epoch 300/300 Batch: 4 Cost: 0.000196\n",
      "Epoch 300/300 Batch: 6 Cost: 0.000299\n",
      "Epoch 300/300 Batch: 8 Cost: 0.000177\n",
      "Epoch 300/300 Batch: 10 Cost: 0.000257\n",
      "Epoch 300/300 Batch: 12 Cost: 0.000157\n",
      "Epoch 300/300 Batch: 14 Cost: 0.000432\n",
      "Epoch 300/300 Batch: 16 Cost: 0.000218\n",
      "Epoch 300/300 Batch: 18 Cost: 0.000362\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 300\n",
    "prunFreq = 1\n",
    "\n",
    "\"\"\"\n",
    "Training\n",
    "\"\"\"\n",
    "model = TextLSTM()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "for epoch in range(n_epochs+1):\n",
    "  for batch_idx, samples in enumerate(train_dataloader):\n",
    "\n",
    "    k_train_mb, a_train_mb, y_train_mb = samples \n",
    "\n",
    "    hidden_k = torch.zeros(1, batch_size, hidden_size, requires_grad=True)\n",
    "    cell_k = torch.zeros(1, batch_size, hidden_size, requires_grad=True)\n",
    "    hidden_a = torch.zeros(1, batch_size, hidden_size, requires_grad=True)\n",
    "    cell_a = torch.zeros(1, batch_size, hidden_size, requires_grad=True)\n",
    "\n",
    "    # Forward\n",
    "    output = model((hidden_k, cell_k), (hidden_a, cell_a), (k_train_mb,a_train_mb))\n",
    "\n",
    "    # Cost\n",
    "    loss = criterion(output, y_train_mb.squeeze())\n",
    "\n",
    "    if (epoch) % 50 == 0 and batch_idx % 2 == 0:\n",
    "      # print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.6f}'.format(loss))\n",
    "      print('Epoch {:3d}/{} Batch: {:2d} Cost: {:.6f}'.format(epoch, n_epochs, batch_idx, loss))\n",
    "    \n",
    "    # Backpropagate\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('mac_cpu')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a25b673604e404bbe71cb44188daddb26f9dca9dc7a0ddb839fa50ca7e9deea0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
