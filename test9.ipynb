{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2022/09/02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/goldenyoo/miniforge3/envs/mac_cpu/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.utils.prune as prune\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "from scipy import io\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "# wandb.init(project=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_config = {\n",
    "    'method': 'grid'\n",
    "    }\n",
    "\n",
    "parameters_dict = {\n",
    "    'subject_label': {\n",
    "        'values': [1,2,3,4,5,6,7,8,9]\n",
    "      },\n",
    "    'chop': {\n",
    "        'values': [64,32,16]\n",
    "      },  \n",
    "    'hidden_size': {\n",
    "        'values': [16,8,2]\n",
    "        },\n",
    "    'batch_size': {\n",
    "        'values': [64,128]\n",
    "        },\n",
    "    'optimizer': {\n",
    "        'values': ['adam']\n",
    "        },\n",
    "    'epochs': {\n",
    "        'values': [500]\n",
    "        },\n",
    "    'learning_rate': {\n",
    "        'values': [0.1]\n",
    "      },\n",
    "}\n",
    "\n",
    "sweep_config['parameters'] = parameters_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: t4zx4ms2\n",
      "Sweep URL: https://wandb.ai/goldenyoo/Calib_data_Aug_5_5_25_mac/sweeps/t4zx4ms2\n"
     ]
    }
   ],
   "source": [
    "sweep_id = wandb.sweep(sweep_config, project=\"Calib_data_Aug_5_5_25_mac\")\n",
    "config = wandb.config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MAT file에서 data 읽고 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mat_file_train(num_subject, chop, k_group, a_group, y_group):\n",
    "    mat_file = io.loadmat('/Users/goldenyoo/Library/Mobile Documents/com~apple~CloudDocs/BioCAS_prepare/BCIIV_2a_mat/myData/Aug/Calib_data_'+ str(num_subject) +'_chop_'+str(chop) +'.mat')\n",
    "    # mat_file = io.loadmat('C:/Users/Peter/iCloudDrive/BioCAS_prepare/BCIIV_2a_mat/myData/Aug/Calib_data_'+ str(num_subject) +'_chop_'+str(chop) +'.mat')\n",
    "    \n",
    "\n",
    "    K1 = mat_file['K1']\n",
    "    A1 = mat_file['A1']\n",
    "\n",
    "    K2 = mat_file['K2']\n",
    "    A2 = mat_file['A2']\n",
    "\n",
    "    Y1 = mat_file['Y1']\n",
    "    Y2 = mat_file['Y2']\n",
    "\n",
    "    # K 특성에 대한 Class1 vs Class2 Data 가져오기\n",
    "    k1 = torch.FloatTensor(K1)\n",
    "    k1 = k1.transpose(0,2)\n",
    "\n",
    "    k2 = torch.FloatTensor(K2)\n",
    "    k2 = k2.transpose(0,2)\n",
    "\n",
    "    # A 특성에 대한 Class1 vs Class2 Data 가져오기\n",
    "    a1 = torch.FloatTensor(A1)\n",
    "    a1 = a1.transpose(0,2)\n",
    "\n",
    "    a2 = torch.FloatTensor(A2)\n",
    "    a2 = a2.transpose(0,2)\n",
    "\n",
    "    # Y에 대한 Class1 vs Class2 Data 가져오기\n",
    "    y1 = torch.LongTensor(Y1)\n",
    "    y2 = torch.LongTensor(Y2)\n",
    "\n",
    "    k_train = torch.cat([k1,k2],dim=0)\n",
    "    a_train = torch.cat([a1,a2],dim=0)\n",
    "\n",
    "    y_train = torch.cat([y1,y2],dim=0)\n",
    "    y_train = y_train-1 # y를 0~1의 정수로 만들어야함.\n",
    "\n",
    "    #group 만들기\n",
    "    if num_subject == 1:\n",
    "        k_group = k_train.to(device)\n",
    "        a_group = a_train.to(device)\n",
    "        y_group = y_train.to(device)\n",
    "    else:\n",
    "        k_group = torch.cat([k_group.to(device),k_train.to(device)], dim=0).to(device)\n",
    "        a_group = torch.cat([a_group.to(device),a_train.to(device)], dim=0).to(device)\n",
    "        y_group = torch.cat([y_group.to(device),y_train.to(device)], dim=0).to(device)\n",
    "\n",
    "    return k_group, a_group, y_group\n",
    "\n",
    "\n",
    "def load_mat_file_test(num_subject, chop, k_group, a_group, y_group):\n",
    "    mat_file = io.loadmat('/Users/goldenyoo/Library/Mobile Documents/com~apple~CloudDocs/BioCAS_prepare/BCIIV_2a_mat/myData/Aug/Eval_data_'+ str(num_subject) +'_chop_'+str(chop) +'.mat')\n",
    "    # mat_file = io.loadmat('C:/Users/Peter/iCloudDrive/BioCAS_prepare/BCIIV_2a_mat/myData/Aug/Eval_data_'+ str(num_subject) +'_chop_'+str(chop) +'.mat')\n",
    "\n",
    "    K1 = mat_file['K1']\n",
    "    A1 = mat_file['A1']\n",
    "\n",
    "    K2 = mat_file['K2']\n",
    "    A2 = mat_file['A2']\n",
    "\n",
    "    Y1 = mat_file['Y1']\n",
    "    Y2 = mat_file['Y2']\n",
    "\n",
    "    # K 특성에 대한 Class1 vs Class2 Data 가져오기\n",
    "    k1 = torch.FloatTensor(K1)\n",
    "    k1 = k1.transpose(0,2)\n",
    "\n",
    "    k2 = torch.FloatTensor(K2)\n",
    "    k2 = k2.transpose(0,2)\n",
    "\n",
    "    # A 특성에 대한 Class1 vs Class2 Data 가져오기\n",
    "    a1 = torch.FloatTensor(A1)\n",
    "    a1 = a1.transpose(0,2)\n",
    "\n",
    "    a2 = torch.FloatTensor(A2)\n",
    "    a2 = a2.transpose(0,2)\n",
    "\n",
    "    # Y에 대한 Class1 vs Class2 Data 가져오기\n",
    "    y1 = torch.LongTensor(Y1)\n",
    "    y2 = torch.LongTensor(Y2)\n",
    "\n",
    "    k_train = torch.cat([k1,k2],dim=0)\n",
    "    a_train = torch.cat([a1,a2],dim=0)\n",
    "\n",
    "    y_train = torch.cat([y1,y2],dim=0)\n",
    "    y_train = y_train-1 # y를 0~1의 정수로 만들어야함.\n",
    "\n",
    "    #group 만들기\n",
    "    if num_subject == 0:\n",
    "        k_group = k_train.to(device)\n",
    "        a_group = a_train.to(device)\n",
    "        y_group = y_train.to(device)\n",
    "    else:\n",
    "        k_group = torch.cat([k_group.to(device),k_train.to(device)], dim=0)\n",
    "        a_group = torch.cat([a_group.to(device),a_train.to(device)], dim=0)\n",
    "        y_group = torch.cat([y_group.to(device),y_train.to(device)], dim=0)\n",
    "\n",
    "    return k_group, a_group, y_group\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k_train = []\n",
    "# a_train = []\n",
    "# y_train = []\n",
    "\n",
    "# k_test = []\n",
    "# a_test = []\n",
    "# y_test = []\n",
    "\n",
    "# for i in range(1,10):\n",
    "#     k_train, a_train, y_train = load_mat_file_train(i, k_train, a_train, y_train)\n",
    "#     k_test, a_test, y_test = load_mat_file_test(i, k_test, a_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset & DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset(batch_size,k_train, a_train, y_train, k_test, a_test, y_test):\n",
    "    dataset_train = TensorDataset(k_train.to(device)   ,a_train.to(device),   y_train.to(device)) # 각 tensor의 첫번째 dim이 일치해야한다\n",
    "    dataset_test  = TensorDataset(k_test.to(device)    ,a_test.to(device) ,   y_test.to(device) ) # 각 tensor의 첫번째 dim이 일치해야한다\n",
    "\n",
    "    # Data Split\n",
    "    dataset_size = len(dataset_train)\n",
    "    train_size = int(dataset_size * 0.8)\n",
    "    validation_size = dataset_size - train_size\n",
    "    train_dataset, valid_dataset = random_split(dataset_train, [train_size, validation_size])\n",
    "\n",
    "    train_dataloader    = DataLoader(train_dataset  ,batch_size=      batch_size  , shuffle=True, drop_last=True)\n",
    "    valid_dataloader    = DataLoader(valid_dataset  ,batch_size= validation_size, shuffle=True, drop_last=True)\n",
    "\n",
    "    # Data Split\n",
    "    test_size = len(dataset_test)\n",
    "    test_dataloader = DataLoader(dataset_test, batch_size= test_size, shuffle=True, drop_last=True)\n",
    "\n",
    "    return train_dataloader, valid_dataloader, test_dataloader,  validation_size,test_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_optimizer(network, optimizer, learning_rate):\n",
    "    if optimizer == \"sgd\":\n",
    "        optimizer = optim.SGD(network.parameters(),\n",
    "                              lr=learning_rate, momentum=0.9)\n",
    "    elif optimizer == \"adam\":\n",
    "        optimizer = optim.Adam(network.parameters(),\n",
    "                               lr=learning_rate)\n",
    "    return optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 22\n",
    "n_class = 2\n",
    "\n",
    "dtype = torch.float\n",
    "\n",
    "class TextLSTM(nn.Module):\n",
    "  def __init__(self,hidden_size):\n",
    "    super(TextLSTM, self).__init__()\n",
    "\n",
    "    self.lstm_1 = nn.LSTM(input_size=input_size, hidden_size=hidden_size)\n",
    "    self.lstm_2 = nn.LSTM(input_size=input_size, hidden_size=hidden_size)\n",
    "    self.fc = nn.Linear(hidden_size*2, n_class)\n",
    "\n",
    "  def forward(self, hidden_and_cell_k, hidden_and_cell_a, K_and_A):\n",
    "    (k, a) = K_and_A\n",
    "\n",
    "    k = k.transpose(1,2)\n",
    "    k = k.transpose(0,1)\n",
    "    a = a.transpose(1,2)\n",
    "    a = a.transpose(0,1)\n",
    "\n",
    "    outputs1, (h_n1,c_n1) = self.lstm_1(k, hidden_and_cell_k)\n",
    "    outputs2, (h_n2,c_n2) = self.lstm_2(a, hidden_and_cell_a)\n",
    "\n",
    "    outputs = torch.cat((h_n1[-1],h_n2[-1]), dim=1)  \n",
    "\n",
    "    model = self.fc(outputs)  # 최종 예측 최종 출력 층\n",
    "    return model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(config=None):\n",
    "    # Initialize a new wandb run\n",
    "    with wandb.init(config=config):\n",
    "        # If called by wandb.agent, as below,\n",
    "        # this config will be set by Sweep Controller\n",
    "        config = wandb.config\n",
    "\n",
    "\n",
    "        k_train = torch.tensor([]).to(device)\n",
    "        a_train = torch.tensor([]).to(device)\n",
    "        y_train = torch.LongTensor([]).to(device)\n",
    "\n",
    "        k_test = torch.tensor([]).to(device)\n",
    "        a_test = torch.tensor([]).to(device)\n",
    "        y_test = torch.LongTensor([]).to(device)\n",
    "\n",
    "        k_train, a_train, y_train = load_mat_file_train(config.subject_label, config.chop,k_train, a_train, y_train)\n",
    "        k_test, a_test, y_test = load_mat_file_test(config.subject_label, config.chop, k_test, a_test, y_test)\n",
    "\n",
    "        model = TextLSTM(hidden_size=config.hidden_size).to(device)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = build_optimizer(model, config.optimizer, config.learning_rate)\n",
    "        scheduler = optim.lr_scheduler.LambdaLR(optimizer=optimizer,\n",
    "                                        lr_lambda=lambda epoch: 0.95 ** epoch,\n",
    "                                        last_epoch=-1,\n",
    "                                        verbose=False)\n",
    "\n",
    "        model.train()\n",
    "        \n",
    "        train_dataloader, valid_dataloader, test_dataloader, validation_size,test_size = build_dataset(config.batch_size, k_train.to(device), a_train.to(device), y_train.to(device), k_test.to(device), a_test.to(device), y_test.to(device))\n",
    "\n",
    "        for epoch in range(config.epochs):\n",
    "            model.train()\n",
    "            for batch_idx, samples in enumerate(train_dataloader):\n",
    "\n",
    "                k_train_mb, a_train_mb, y_train_mb = samples\n",
    "\n",
    "                hidden_k  = torch.zeros(1, config.batch_size, config.hidden_size, requires_grad=True).to(device)\n",
    "                cell_k    = torch.zeros(1, config.batch_size, config.hidden_size, requires_grad=True).to(device)\n",
    "                hidden_a  = torch.zeros(1, config.batch_size, config.hidden_size, requires_grad=True).to(device)\n",
    "                cell_a    = torch.zeros(1, config.batch_size, config.hidden_size, requires_grad=True).to(device)\n",
    "\n",
    "                # Forward\n",
    "                output = model((hidden_k, cell_k), (hidden_a, cell_a), (k_train_mb.to(device),a_train_mb.to(device)))\n",
    "\n",
    "                # Cost\n",
    "                loss = criterion(output.to(device), y_train_mb.squeeze().to(device))\n",
    "\n",
    "                wandb.log({\"loss\": loss})\n",
    "\n",
    "                if (epoch) % 100 == 0 and batch_idx % 10 == 0:\n",
    "                    print('Epoch {:3d}/{} Batch: {:2d} Cost: {:.6f}'.format(epoch, config.epochs, batch_idx, loss))\n",
    "                \n",
    "                # Backpropagate\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                for batch_idx, samples in  enumerate(valid_dataloader):\n",
    "                    k_train_mb, a_train_mb, y_train_mb = samples\n",
    "\n",
    "                    hidden_k  = torch.zeros(1, validation_size, config.hidden_size, requires_grad=True).to(device)\n",
    "                    cell_k    = torch.zeros(1, validation_size, config.hidden_size, requires_grad=True).to(device)\n",
    "                    hidden_a  = torch.zeros(1, validation_size, config.hidden_size, requires_grad=True).to(device)\n",
    "                    cell_a    = torch.zeros(1, validation_size, config.hidden_size, requires_grad=True).to(device)\n",
    "\n",
    "                    # Forward\n",
    "                    output = model((hidden_k, cell_k), (hidden_a, cell_a), (k_train_mb.to(device),a_train_mb.to(device)))\n",
    "\n",
    "                    # Cost\n",
    "                    valid_loss = criterion(output.to(device), y_train_mb.squeeze().to(device))\n",
    "                    if (epoch) % 100 == 0:\n",
    "                        print('Epoch {:3d}/{} Valid_loss: {}'.format(epoch, config.epochs,valid_loss))\n",
    "            scheduler.step()\n",
    "        model.eval()\n",
    "\n",
    "        for batch_idx, samples in enumerate(test_dataloader):\n",
    "            k_train_mb, a_train_mb, y_train_mb = samples\n",
    "\n",
    "            hidden_k    = torch.zeros(1, test_size, config.hidden_size).to(device)\n",
    "            cell_k      = torch.zeros(1, test_size, config.hidden_size).to(device)\n",
    "            hidden_a    = torch.zeros(1, test_size, config.hidden_size).to(device)\n",
    "            cell_a      = torch.zeros(1, test_size, config.hidden_size).to(device)\n",
    "\n",
    "            output = model((hidden_k, cell_k), (hidden_a, cell_a), (k_train_mb.to(device),a_train_mb.to(device)))\n",
    "            prediction = output.argmax(dim=1)\n",
    "            correct = prediction.eq(y_train_mb.view_as(prediction)).sum().item()\n",
    "            print(correct/test_size)\n",
    "            wandb.log({\"accuracy\": correct/test_size})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: uzoito07 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchop: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 500\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsubject_label: 1\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mgoldenyoo\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/goldenyoo/Library/Mobile Documents/com~apple~CloudDocs/Pytorch/wandb/run-20220904_171621-uzoito07</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/goldenyoo/Calib_data_Aug_5_5_25_mac/runs/uzoito07\" target=\"_blank\">swift-sweep-1</a></strong> to <a href=\"https://wandb.ai/goldenyoo/Calib_data_Aug_5_5_25_mac\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/goldenyoo/Calib_data_Aug_5_5_25_mac/sweeps/t4zx4ms2\" target=\"_blank\">https://wandb.ai/goldenyoo/Calib_data_Aug_5_5_25_mac/sweeps/t4zx4ms2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   0/500 Batch:  0 Cost: 0.709825\n",
      "Epoch   0/500 Valid_loss: 0.6988968253135681\n",
      "Epoch 100/500 Batch:  0 Cost: 0.578486\n",
      "Epoch 100/500 Valid_loss: 0.7171905636787415\n",
      "Epoch 200/500 Batch:  0 Cost: 0.614582\n",
      "Epoch 200/500 Valid_loss: 0.7176215648651123\n",
      "Epoch 300/500 Batch:  0 Cost: 0.554896\n",
      "Epoch 300/500 Valid_loss: 0.7176233530044556\n",
      "Epoch 400/500 Batch:  0 Cost: 0.744645\n",
      "Epoch 400/500 Valid_loss: 0.7176234722137451\n",
      "0.4664351851851852\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>▁</td></tr><tr><td>loss</td><td>▆▅▅▃▆▃▄▂▃▃▄▃▃▄▃▆▂▁▂▆▃▂▄▃▃▄▁▇▃▄▄▃▂▅█▂▃▅▃▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>0.46644</td></tr><tr><td>loss</td><td>0.59629</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">swift-sweep-1</strong>: <a href=\"https://wandb.ai/goldenyoo/Calib_data_Aug_5_5_25_mac/runs/uzoito07\" target=\"_blank\">https://wandb.ai/goldenyoo/Calib_data_Aug_5_5_25_mac/runs/uzoito07</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220904_171621-uzoito07/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: db1yhcao with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchop: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 500\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsubject_label: 2\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Ctrl + C detected. Stopping sweep.\n"
     ]
    }
   ],
   "source": [
    "wandb.agent(sweep_id, train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('mac_cpu')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a25b673604e404bbe71cb44188daddb26f9dca9dc7a0ddb839fa50ca7e9deea0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
