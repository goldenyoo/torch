{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2022/08/31"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/goldenyoo/miniforge3/envs/mac_cpu/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.utils.prune as prune\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from scipy import io\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MAT file에서 data 읽고 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat_file = io.loadmat('/Users/goldenyoo/Library/Mobile Documents/com~apple~CloudDocs/BioCAS_prepare/Python_code/Data_center/one_dx/Calib_data_1.mat')\n",
    "\n",
    "K1 = mat_file['K1']\n",
    "A1 = mat_file['A1']\n",
    "\n",
    "K2 = mat_file['K2']\n",
    "A2 = mat_file['A2']\n",
    "\n",
    "Y1 = mat_file['Y1']\n",
    "Y2 = mat_file['Y2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total_data: 828 / input_size: 22 / Seq_len: 16\n",
      "\n",
      "k1 size:  torch.Size([828, 22, 16])\n",
      "k2 size:  torch.Size([828, 22, 16])\n",
      "a1 size:  torch.Size([828, 22, 16])\n",
      "a2 size:  torch.Size([828, 22, 16])\n",
      "\n",
      "y1 size: torch.Size([828, 1])\n",
      "y2 size: torch.Size([828, 1])\n"
     ]
    }
   ],
   "source": [
    "# K 특성에 대한 Class1 vs Class2 Data 가져오기\n",
    "k1 = torch.FloatTensor(K1)\n",
    "k1 = k1.transpose(0,2)\n",
    "\n",
    "k2 = torch.FloatTensor(K2)\n",
    "k2 = k2.transpose(0,2)\n",
    "\n",
    "# A 특성에 대한 Class1 vs Class2 Data 가져오기\n",
    "a1 = torch.FloatTensor(A1)\n",
    "a1 = a1.transpose(0,2)\n",
    "\n",
    "a2 = torch.FloatTensor(A2)\n",
    "a2 = a2.transpose(0,2)\n",
    "\n",
    "print( \"Total_data: {}\".format(k1.size()[0]), \"/ input_size: {}\".format(k1.size()[1]),\"/ Seq_len: {}\\n\".format(k1.size()[2]))\n",
    "print(\"k1 size: \",k1.size())\n",
    "print(\"k2 size: \",k2.size())\n",
    "print(\"a1 size: \",a1.size())\n",
    "print(\"a2 size: \",a2.size())\n",
    "\n",
    "# Y에 대한 Class1 vs Class2 Data 가져오기\n",
    "y1 = torch.LongTensor(Y1)\n",
    "y2 = torch.LongTensor(Y2)\n",
    "\n",
    "print(\"\\ny1 size:\",y1.size())\n",
    "print(\"y2 size:\",y2.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train data 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k_train size: torch.Size([1656, 22, 16])\n",
      "a_train size: torch.Size([1656, 22, 16])\n",
      "y_train size: torch.Size([1656, 1])\n"
     ]
    }
   ],
   "source": [
    "k_train = torch.cat([k1,k2],dim=0)\n",
    "a_train = torch.cat([a1,a2],dim=0)\n",
    "\n",
    "y_train = torch.cat([y1,y2],dim=0)\n",
    "print(\"k_train size: {}\".format(k_train.size()))\n",
    "print(\"a_train size: {}\".format(a_train.size()))\n",
    "print(\"y_train size: {}\".format(y_train.size()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### y_train의 one-hot coding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1656, 1])\n"
     ]
    }
   ],
   "source": [
    "# y_train_one_hot = F.one_hot(y_train-1,num_classes=2)\n",
    "# print(y_train_one_hot.size())\n",
    "# y_train_one_hot.squeeze_()\n",
    "# print(y_train_one_hot.size())\n",
    "\n",
    "y_train = y_train-1 # y를 0~1의 정수로 만들어야함.\n",
    "print(y_train.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset & DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "dataset = TensorDataset(k_train,a_train,y_train) # 각 tensor의 첫번째 dim이 일치해야한다\n",
    "dataloader = DataLoader(dataset,batch_size=batch_size, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 3\n",
    "lstm_output_size = hidden_size\n",
    "input_size = 22\n",
    "n_class = 2\n",
    "\n",
    "dtype = torch.float\n",
    "\n",
    "class TextLSTM(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(TextLSTM, self).__init__()\n",
    "\n",
    "    self.lstm_1 = nn.LSTM(input_size=input_size, hidden_size=hidden_size, dropout=0.3)\n",
    "    self.lstm_2 = nn.LSTM(input_size=input_size, hidden_size=hidden_size, dropout=0.3)\n",
    "    self.fc = nn.Linear(hidden_size*2, n_class)\n",
    "\n",
    "  def forward(self, hidden_and_cell_k, hidden_and_cell_a, K_and_A):\n",
    "    (k, a) = K_and_A\n",
    "\n",
    "\n",
    "\n",
    "    k = k.transpose(1,2)\n",
    "    k = k.transpose(0,1)\n",
    "    a = a.transpose(1,2)\n",
    "    a = a.transpose(0,1)\n",
    "\n",
    "    outputs1, (h_n1,c_n1) = self.lstm_1(k, hidden_and_cell_k)\n",
    "    outputs2, (h_n2,c_n2) = self.lstm_2(a, hidden_and_cell_a)\n",
    "\n",
    "    outputs = torch.cat((h_n1[-1],h_n2[-1]), dim=1)  \n",
    "\n",
    "    model = self.fc(outputs)  # 최종 예측 최종 출력 층\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TextLSTM(\n",
      "  (lstm_1): LSTM(22, 3, dropout=0.3)\n",
      "  (lstm_2): LSTM(22, 3, dropout=0.3)\n",
      "  (fc): Linear(in_features=6, out_features=2, bias=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/goldenyoo/miniforge3/envs/mac_cpu/lib/python3.8/site-packages/torch/nn/modules/rnn.py:62: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    }
   ],
   "source": [
    "model = TextLSTM()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/goldenyoo/miniforge3/envs/mac_cpu/lib/python3.8/site-packages/torch/nn/modules/rnn.py:62: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1/500 Batch: 1 Cost: 0.729939\n",
      "Epoch   1/500 Batch: 3 Cost: 0.702374\n",
      "Epoch   1/500 Batch: 5 Cost: 0.687833\n",
      "Epoch   1/500 Batch: 7 Cost: 0.731255\n",
      "Epoch   1/500 Batch: 9 Cost: 0.681396\n",
      "Epoch   1/500 Batch: 11 Cost: 0.690396\n",
      "Epoch  11/500 Batch: 1 Cost: 0.518629\n",
      "Epoch  11/500 Batch: 3 Cost: 0.603890\n",
      "Epoch  11/500 Batch: 5 Cost: 0.483699\n",
      "Epoch  11/500 Batch: 7 Cost: 0.475673\n",
      "Epoch  11/500 Batch: 9 Cost: 0.556618\n",
      "Epoch  11/500 Batch: 11 Cost: 0.485907\n",
      "Epoch  21/500 Batch: 1 Cost: 0.224956\n",
      "Epoch  21/500 Batch: 3 Cost: 0.152927\n",
      "Epoch  21/500 Batch: 5 Cost: 0.157804\n",
      "Epoch  21/500 Batch: 7 Cost: 0.179800\n",
      "Epoch  21/500 Batch: 9 Cost: 0.170132\n",
      "Epoch  21/500 Batch: 11 Cost: 0.248025\n",
      "Epoch  31/500 Batch: 1 Cost: 0.084414\n",
      "Epoch  31/500 Batch: 3 Cost: 0.066422\n",
      "Epoch  31/500 Batch: 5 Cost: 0.061862\n",
      "Epoch  31/500 Batch: 7 Cost: 0.086780\n",
      "Epoch  31/500 Batch: 9 Cost: 0.098657\n",
      "Epoch  31/500 Batch: 11 Cost: 0.062788\n",
      "Epoch  41/500 Batch: 1 Cost: 0.044391\n",
      "Epoch  41/500 Batch: 3 Cost: 0.039390\n",
      "Epoch  41/500 Batch: 5 Cost: 0.037037\n",
      "Epoch  41/500 Batch: 7 Cost: 0.031402\n",
      "Epoch  41/500 Batch: 9 Cost: 0.033670\n",
      "Epoch  41/500 Batch: 11 Cost: 0.032590\n",
      "Epoch  51/500 Batch: 1 Cost: 0.022969\n",
      "Epoch  51/500 Batch: 3 Cost: 0.017085\n",
      "Epoch  51/500 Batch: 5 Cost: 0.023449\n",
      "Epoch  51/500 Batch: 7 Cost: 0.015184\n",
      "Epoch  51/500 Batch: 9 Cost: 0.020103\n",
      "Epoch  51/500 Batch: 11 Cost: 0.015365\n",
      "Epoch  61/500 Batch: 1 Cost: 0.014424\n",
      "Epoch  61/500 Batch: 3 Cost: 0.010734\n",
      "Epoch  61/500 Batch: 5 Cost: 0.013012\n",
      "Epoch  61/500 Batch: 7 Cost: 0.013605\n",
      "Epoch  61/500 Batch: 9 Cost: 0.018079\n",
      "Epoch  61/500 Batch: 11 Cost: 0.013386\n",
      "Epoch  71/500 Batch: 1 Cost: 0.009806\n",
      "Epoch  71/500 Batch: 3 Cost: 0.008896\n",
      "Epoch  71/500 Batch: 5 Cost: 0.009388\n",
      "Epoch  71/500 Batch: 7 Cost: 0.007907\n",
      "Epoch  71/500 Batch: 9 Cost: 0.007399\n",
      "Epoch  71/500 Batch: 11 Cost: 0.008269\n",
      "Epoch  81/500 Batch: 1 Cost: 0.007711\n",
      "Epoch  81/500 Batch: 3 Cost: 0.008615\n",
      "Epoch  81/500 Batch: 5 Cost: 0.004920\n",
      "Epoch  81/500 Batch: 7 Cost: 0.009454\n",
      "Epoch  81/500 Batch: 9 Cost: 0.009715\n",
      "Epoch  81/500 Batch: 11 Cost: 0.007376\n",
      "Epoch  91/500 Batch: 1 Cost: 0.006773\n",
      "Epoch  91/500 Batch: 3 Cost: 0.005265\n",
      "Epoch  91/500 Batch: 5 Cost: 0.005289\n",
      "Epoch  91/500 Batch: 7 Cost: 0.005265\n",
      "Epoch  91/500 Batch: 9 Cost: 0.008999\n",
      "Epoch  91/500 Batch: 11 Cost: 0.006756\n",
      "Epoch 101/500 Batch: 1 Cost: 0.004844\n",
      "Epoch 101/500 Batch: 3 Cost: 0.004467\n",
      "Epoch 101/500 Batch: 5 Cost: 0.004319\n",
      "Epoch 101/500 Batch: 7 Cost: 0.004620\n",
      "Epoch 101/500 Batch: 9 Cost: 0.006876\n",
      "Epoch 101/500 Batch: 11 Cost: 0.005335\n",
      "Epoch 111/500 Batch: 1 Cost: 0.003709\n",
      "Epoch 111/500 Batch: 3 Cost: 0.003224\n",
      "Epoch 111/500 Batch: 5 Cost: 0.004286\n",
      "Epoch 111/500 Batch: 7 Cost: 0.005416\n",
      "Epoch 111/500 Batch: 9 Cost: 0.003361\n",
      "Epoch 111/500 Batch: 11 Cost: 0.003155\n",
      "Epoch 121/500 Batch: 1 Cost: 0.003735\n",
      "Epoch 121/500 Batch: 3 Cost: 0.002931\n",
      "Epoch 121/500 Batch: 5 Cost: 0.002656\n",
      "Epoch 121/500 Batch: 7 Cost: 0.003408\n",
      "Epoch 121/500 Batch: 9 Cost: 0.002414\n",
      "Epoch 121/500 Batch: 11 Cost: 0.002853\n",
      "Epoch 131/500 Batch: 1 Cost: 0.002579\n",
      "Epoch 131/500 Batch: 3 Cost: 0.002484\n",
      "Epoch 131/500 Batch: 5 Cost: 0.002282\n",
      "Epoch 131/500 Batch: 7 Cost: 0.002933\n",
      "Epoch 131/500 Batch: 9 Cost: 0.003144\n",
      "Epoch 131/500 Batch: 11 Cost: 0.001910\n",
      "Epoch 141/500 Batch: 1 Cost: 0.001496\n",
      "Epoch 141/500 Batch: 3 Cost: 0.002170\n",
      "Epoch 141/500 Batch: 5 Cost: 0.002421\n",
      "Epoch 141/500 Batch: 7 Cost: 0.001370\n",
      "Epoch 141/500 Batch: 9 Cost: 0.001401\n",
      "Epoch 141/500 Batch: 11 Cost: 0.003038\n",
      "Epoch 151/500 Batch: 1 Cost: 0.001879\n",
      "Epoch 151/500 Batch: 3 Cost: 0.002291\n",
      "Epoch 151/500 Batch: 5 Cost: 0.001863\n",
      "Epoch 151/500 Batch: 7 Cost: 0.001655\n",
      "Epoch 151/500 Batch: 9 Cost: 0.001513\n",
      "Epoch 151/500 Batch: 11 Cost: 0.001809\n",
      "Epoch 161/500 Batch: 1 Cost: 0.001489\n",
      "Epoch 161/500 Batch: 3 Cost: 0.001645\n",
      "Epoch 161/500 Batch: 5 Cost: 0.001359\n",
      "Epoch 161/500 Batch: 7 Cost: 0.001950\n",
      "Epoch 161/500 Batch: 9 Cost: 0.001447\n",
      "Epoch 161/500 Batch: 11 Cost: 0.001388\n",
      "Epoch 171/500 Batch: 1 Cost: 0.001338\n",
      "Epoch 171/500 Batch: 3 Cost: 0.001433\n",
      "Epoch 171/500 Batch: 5 Cost: 0.001382\n",
      "Epoch 171/500 Batch: 7 Cost: 0.002257\n",
      "Epoch 171/500 Batch: 9 Cost: 0.001084\n",
      "Epoch 171/500 Batch: 11 Cost: 0.001499\n",
      "Epoch 181/500 Batch: 1 Cost: 0.001138\n",
      "Epoch 181/500 Batch: 3 Cost: 0.001252\n",
      "Epoch 181/500 Batch: 5 Cost: 0.001337\n",
      "Epoch 181/500 Batch: 7 Cost: 0.002038\n",
      "Epoch 181/500 Batch: 9 Cost: 0.001344\n",
      "Epoch 181/500 Batch: 11 Cost: 0.000804\n",
      "Epoch 191/500 Batch: 1 Cost: 0.000832\n",
      "Epoch 191/500 Batch: 3 Cost: 0.001595\n",
      "Epoch 191/500 Batch: 5 Cost: 0.001063\n",
      "Epoch 191/500 Batch: 7 Cost: 0.000833\n",
      "Epoch 191/500 Batch: 9 Cost: 0.000897\n",
      "Epoch 191/500 Batch: 11 Cost: 0.001401\n",
      "Epoch 201/500 Batch: 1 Cost: 0.001128\n",
      "Epoch 201/500 Batch: 3 Cost: 0.000808\n",
      "Epoch 201/500 Batch: 5 Cost: 0.001416\n",
      "Epoch 201/500 Batch: 7 Cost: 0.000904\n",
      "Epoch 201/500 Batch: 9 Cost: 0.001010\n",
      "Epoch 201/500 Batch: 11 Cost: 0.000908\n",
      "Epoch 211/500 Batch: 1 Cost: 0.000794\n",
      "Epoch 211/500 Batch: 3 Cost: 0.000600\n",
      "Epoch 211/500 Batch: 5 Cost: 0.001210\n",
      "Epoch 211/500 Batch: 7 Cost: 0.000976\n",
      "Epoch 211/500 Batch: 9 Cost: 0.000700\n",
      "Epoch 211/500 Batch: 11 Cost: 0.001141\n",
      "Epoch 221/500 Batch: 1 Cost: 0.001225\n",
      "Epoch 221/500 Batch: 3 Cost: 0.001019\n",
      "Epoch 221/500 Batch: 5 Cost: 0.000790\n",
      "Epoch 221/500 Batch: 7 Cost: 0.000894\n",
      "Epoch 221/500 Batch: 9 Cost: 0.000823\n",
      "Epoch 221/500 Batch: 11 Cost: 0.000850\n",
      "Epoch 231/500 Batch: 1 Cost: 0.058752\n",
      "Epoch 231/500 Batch: 3 Cost: 0.009534\n",
      "Epoch 231/500 Batch: 5 Cost: 0.054816\n",
      "Epoch 231/500 Batch: 7 Cost: 0.104979\n",
      "Epoch 231/500 Batch: 9 Cost: 0.050965\n",
      "Epoch 231/500 Batch: 11 Cost: 0.028524\n",
      "Epoch 241/500 Batch: 1 Cost: 0.003094\n",
      "Epoch 241/500 Batch: 3 Cost: 0.002066\n",
      "Epoch 241/500 Batch: 5 Cost: 0.002891\n",
      "Epoch 241/500 Batch: 7 Cost: 0.002126\n",
      "Epoch 241/500 Batch: 9 Cost: 0.002591\n",
      "Epoch 241/500 Batch: 11 Cost: 0.002493\n",
      "Epoch 251/500 Batch: 1 Cost: 0.001802\n",
      "Epoch 251/500 Batch: 3 Cost: 0.001899\n",
      "Epoch 251/500 Batch: 5 Cost: 0.001499\n",
      "Epoch 251/500 Batch: 7 Cost: 0.001500\n",
      "Epoch 251/500 Batch: 9 Cost: 0.000899\n",
      "Epoch 251/500 Batch: 11 Cost: 0.001551\n",
      "Epoch 261/500 Batch: 1 Cost: 0.001511\n",
      "Epoch 261/500 Batch: 3 Cost: 0.001279\n",
      "Epoch 261/500 Batch: 5 Cost: 0.001340\n",
      "Epoch 261/500 Batch: 7 Cost: 0.001023\n",
      "Epoch 261/500 Batch: 9 Cost: 0.001087\n",
      "Epoch 261/500 Batch: 11 Cost: 0.001035\n",
      "Epoch 271/500 Batch: 1 Cost: 0.001222\n",
      "Epoch 271/500 Batch: 3 Cost: 0.000996\n",
      "Epoch 271/500 Batch: 5 Cost: 0.001070\n",
      "Epoch 271/500 Batch: 7 Cost: 0.001032\n",
      "Epoch 271/500 Batch: 9 Cost: 0.000689\n",
      "Epoch 271/500 Batch: 11 Cost: 0.000972\n",
      "Epoch 281/500 Batch: 1 Cost: 0.000431\n",
      "Epoch 281/500 Batch: 3 Cost: 0.000822\n",
      "Epoch 281/500 Batch: 5 Cost: 0.001104\n",
      "Epoch 281/500 Batch: 7 Cost: 0.001348\n",
      "Epoch 281/500 Batch: 9 Cost: 0.000800\n",
      "Epoch 281/500 Batch: 11 Cost: 0.000798\n",
      "Epoch 291/500 Batch: 1 Cost: 0.001189\n",
      "Epoch 291/500 Batch: 3 Cost: 0.000631\n",
      "Epoch 291/500 Batch: 5 Cost: 0.000617\n",
      "Epoch 291/500 Batch: 7 Cost: 0.000511\n",
      "Epoch 291/500 Batch: 9 Cost: 0.000774\n",
      "Epoch 291/500 Batch: 11 Cost: 0.000969\n",
      "Epoch 301/500 Batch: 1 Cost: 0.000989\n",
      "Epoch 301/500 Batch: 3 Cost: 0.000385\n",
      "Epoch 301/500 Batch: 5 Cost: 0.000699\n",
      "Epoch 301/500 Batch: 7 Cost: 0.000736\n",
      "Epoch 301/500 Batch: 9 Cost: 0.000344\n",
      "Epoch 301/500 Batch: 11 Cost: 0.000764\n",
      "Epoch 311/500 Batch: 1 Cost: 0.000728\n",
      "Epoch 311/500 Batch: 3 Cost: 0.000675\n",
      "Epoch 311/500 Batch: 5 Cost: 0.000383\n",
      "Epoch 311/500 Batch: 7 Cost: 0.000537\n",
      "Epoch 311/500 Batch: 9 Cost: 0.000887\n",
      "Epoch 311/500 Batch: 11 Cost: 0.000615\n",
      "Epoch 321/500 Batch: 1 Cost: 0.000452\n",
      "Epoch 321/500 Batch: 3 Cost: 0.000551\n",
      "Epoch 321/500 Batch: 5 Cost: 0.000483\n",
      "Epoch 321/500 Batch: 7 Cost: 0.000261\n",
      "Epoch 321/500 Batch: 9 Cost: 0.000709\n",
      "Epoch 321/500 Batch: 11 Cost: 0.000517\n",
      "Epoch 331/500 Batch: 1 Cost: 0.000564\n",
      "Epoch 331/500 Batch: 3 Cost: 0.000250\n",
      "Epoch 331/500 Batch: 5 Cost: 0.000681\n",
      "Epoch 331/500 Batch: 7 Cost: 0.000416\n",
      "Epoch 331/500 Batch: 9 Cost: 0.000553\n",
      "Epoch 331/500 Batch: 11 Cost: 0.000474\n",
      "Epoch 341/500 Batch: 1 Cost: 0.000329\n",
      "Epoch 341/500 Batch: 3 Cost: 0.000477\n",
      "Epoch 341/500 Batch: 5 Cost: 0.000568\n",
      "Epoch 341/500 Batch: 7 Cost: 0.000470\n",
      "Epoch 341/500 Batch: 9 Cost: 0.000347\n",
      "Epoch 341/500 Batch: 11 Cost: 0.000332\n",
      "Epoch 351/500 Batch: 1 Cost: 0.000570\n",
      "Epoch 351/500 Batch: 3 Cost: 0.000541\n",
      "Epoch 351/500 Batch: 5 Cost: 0.000252\n",
      "Epoch 351/500 Batch: 7 Cost: 0.000225\n",
      "Epoch 351/500 Batch: 9 Cost: 0.000505\n",
      "Epoch 351/500 Batch: 11 Cost: 0.000341\n",
      "Epoch 361/500 Batch: 1 Cost: 0.000424\n",
      "Epoch 361/500 Batch: 3 Cost: 0.000319\n",
      "Epoch 361/500 Batch: 5 Cost: 0.000548\n",
      "Epoch 361/500 Batch: 7 Cost: 0.000291\n",
      "Epoch 361/500 Batch: 9 Cost: 0.000348\n",
      "Epoch 361/500 Batch: 11 Cost: 0.000522\n",
      "Epoch 371/500 Batch: 1 Cost: 0.000399\n",
      "Epoch 371/500 Batch: 3 Cost: 0.000349\n",
      "Epoch 371/500 Batch: 5 Cost: 0.000283\n",
      "Epoch 371/500 Batch: 7 Cost: 0.000531\n",
      "Epoch 371/500 Batch: 9 Cost: 0.000589\n",
      "Epoch 371/500 Batch: 11 Cost: 0.000299\n",
      "Epoch 381/500 Batch: 1 Cost: 0.000410\n",
      "Epoch 381/500 Batch: 3 Cost: 0.000423\n",
      "Epoch 381/500 Batch: 5 Cost: 0.000355\n",
      "Epoch 381/500 Batch: 7 Cost: 0.000239\n",
      "Epoch 381/500 Batch: 9 Cost: 0.000232\n",
      "Epoch 381/500 Batch: 11 Cost: 0.000230\n",
      "Epoch 391/500 Batch: 1 Cost: 0.000458\n",
      "Epoch 391/500 Batch: 3 Cost: 0.000381\n",
      "Epoch 391/500 Batch: 5 Cost: 0.000304\n",
      "Epoch 391/500 Batch: 7 Cost: 0.000319\n",
      "Epoch 391/500 Batch: 9 Cost: 0.000252\n",
      "Epoch 391/500 Batch: 11 Cost: 0.000375\n",
      "Epoch 401/500 Batch: 1 Cost: 0.000246\n",
      "Epoch 401/500 Batch: 3 Cost: 0.000217\n",
      "Epoch 401/500 Batch: 5 Cost: 0.000199\n",
      "Epoch 401/500 Batch: 7 Cost: 0.000306\n",
      "Epoch 401/500 Batch: 9 Cost: 0.000465\n",
      "Epoch 401/500 Batch: 11 Cost: 0.000380\n",
      "Epoch 411/500 Batch: 1 Cost: 0.000169\n",
      "Epoch 411/500 Batch: 3 Cost: 0.000358\n",
      "Epoch 411/500 Batch: 5 Cost: 0.000278\n",
      "Epoch 411/500 Batch: 7 Cost: 0.000130\n",
      "Epoch 411/500 Batch: 9 Cost: 0.000157\n",
      "Epoch 411/500 Batch: 11 Cost: 0.000240\n",
      "Epoch 421/500 Batch: 1 Cost: 0.000232\n",
      "Epoch 421/500 Batch: 3 Cost: 0.000207\n",
      "Epoch 421/500 Batch: 5 Cost: 0.000479\n",
      "Epoch 421/500 Batch: 7 Cost: 0.000282\n",
      "Epoch 421/500 Batch: 9 Cost: 0.000298\n",
      "Epoch 421/500 Batch: 11 Cost: 0.000186\n",
      "Epoch 431/500 Batch: 1 Cost: 0.000184\n",
      "Epoch 431/500 Batch: 3 Cost: 0.000196\n",
      "Epoch 431/500 Batch: 5 Cost: 0.000149\n",
      "Epoch 431/500 Batch: 7 Cost: 0.000170\n",
      "Epoch 431/500 Batch: 9 Cost: 0.000124\n",
      "Epoch 431/500 Batch: 11 Cost: 0.000137\n",
      "Epoch 441/500 Batch: 1 Cost: 0.000262\n",
      "Epoch 441/500 Batch: 3 Cost: 0.000297\n",
      "Epoch 441/500 Batch: 5 Cost: 0.000168\n",
      "Epoch 441/500 Batch: 7 Cost: 0.000120\n",
      "Epoch 441/500 Batch: 9 Cost: 0.000118\n",
      "Epoch 441/500 Batch: 11 Cost: 0.000103\n",
      "Epoch 451/500 Batch: 1 Cost: 0.000219\n",
      "Epoch 451/500 Batch: 3 Cost: 0.000146\n",
      "Epoch 451/500 Batch: 5 Cost: 0.000149\n",
      "Epoch 451/500 Batch: 7 Cost: 0.000187\n",
      "Epoch 451/500 Batch: 9 Cost: 0.000204\n",
      "Epoch 451/500 Batch: 11 Cost: 0.000168\n",
      "Epoch 461/500 Batch: 1 Cost: 0.000310\n",
      "Epoch 461/500 Batch: 3 Cost: 0.000223\n",
      "Epoch 461/500 Batch: 5 Cost: 0.000212\n",
      "Epoch 461/500 Batch: 7 Cost: 0.000148\n",
      "Epoch 461/500 Batch: 9 Cost: 0.000126\n",
      "Epoch 461/500 Batch: 11 Cost: 0.000259\n",
      "Epoch 471/500 Batch: 1 Cost: 0.000139\n",
      "Epoch 471/500 Batch: 3 Cost: 0.000212\n",
      "Epoch 471/500 Batch: 5 Cost: 0.000200\n",
      "Epoch 471/500 Batch: 7 Cost: 0.000162\n",
      "Epoch 471/500 Batch: 9 Cost: 0.000167\n",
      "Epoch 471/500 Batch: 11 Cost: 0.000161\n",
      "Epoch 481/500 Batch: 1 Cost: 0.000137\n",
      "Epoch 481/500 Batch: 3 Cost: 0.000137\n",
      "Epoch 481/500 Batch: 5 Cost: 0.000189\n",
      "Epoch 481/500 Batch: 7 Cost: 0.000204\n",
      "Epoch 481/500 Batch: 9 Cost: 0.000111\n",
      "Epoch 481/500 Batch: 11 Cost: 0.000205\n",
      "Epoch 491/500 Batch: 1 Cost: 0.000155\n",
      "Epoch 491/500 Batch: 3 Cost: 0.000164\n",
      "Epoch 491/500 Batch: 5 Cost: 0.000203\n",
      "Epoch 491/500 Batch: 7 Cost: 0.000181\n",
      "Epoch 491/500 Batch: 9 Cost: 0.000150\n",
      "Epoch 491/500 Batch: 11 Cost: 0.000182\n",
      "Epoch 501/500 Batch: 1 Cost: 0.000214\n",
      "Epoch 501/500 Batch: 3 Cost: 0.000176\n",
      "Epoch 501/500 Batch: 5 Cost: 0.000124\n",
      "Epoch 501/500 Batch: 7 Cost: 0.000087\n",
      "Epoch 501/500 Batch: 9 Cost: 0.000102\n",
      "Epoch 501/500 Batch: 11 Cost: 0.000162\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 500\n",
    "prunFreq = 1\n",
    "\n",
    "\"\"\"\n",
    "Training\n",
    "\"\"\"\n",
    "model = TextLSTM()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "for epoch in range(n_epochs+1):\n",
    "  for batch_idx, samples in enumerate(dataloader):\n",
    "\n",
    "    k_train_mb, a_train_mb, y_train_mb = samples \n",
    "\n",
    "    hidden_k = torch.zeros(1, batch_size, hidden_size, requires_grad=True)\n",
    "    cell_k = torch.zeros(1, batch_size, hidden_size, requires_grad=True)\n",
    "    hidden_a = torch.zeros(1, batch_size, hidden_size, requires_grad=True)\n",
    "    cell_a = torch.zeros(1, batch_size, hidden_size, requires_grad=True)\n",
    "\n",
    "    # Forward\n",
    "    output = model((hidden_k, cell_k), (hidden_a, cell_a), (k_train_mb,a_train_mb))\n",
    "\n",
    "    # Cost\n",
    "    loss = criterion(output, y_train_mb.squeeze())\n",
    "\n",
    "    if (epoch) % 50 == 0 and batch_idx % 2 == 0:\n",
    "      # print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.6f}'.format(loss))\n",
    "      print('Epoch {:3d}/{} Batch: {} Cost: {:.6f}'.format(epoch, n_epochs, batch_idx, loss))\n",
    "    \n",
    "    # Backpropagate\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('mac_cpu')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a25b673604e404bbe71cb44188daddb26f9dca9dc7a0ddb839fa50ca7e9deea0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
