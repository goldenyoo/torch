{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2022/09/06"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.utils.prune as prune\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "from scipy import io\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "# wandb.init(project=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_config = {\n",
    "    'method': 'grid'\n",
    "    }\n",
    "\n",
    "parameters_dict = {\n",
    "    'subject_label': {\n",
    "        'values': [1,2,3,4,5,6,7,8,9]\n",
    "      },\n",
    "    'hidden_size': {\n",
    "        'values': [16,32,64]\n",
    "        },\n",
    "    'batch_size': {\n",
    "        'values': [128]\n",
    "        },\n",
    "    'optimizer': {\n",
    "        'values': ['sgd','adam']\n",
    "        },\n",
    "    'epochs': {\n",
    "        'values': [500]\n",
    "        },\n",
    "    'learning_rate': {\n",
    "        'values': [0.001,0.01,0.1]\n",
    "      },\n",
    "    'chop': {\n",
    "        'values': [16]\n",
    "      },  \n",
    "    }\n",
    "sweep_config['parameters'] = parameters_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: 5ph3xquz\n",
      "Sweep URL: https://wandb.ai/goldenyoo/Calib_eval_divide_Raw_2/sweeps/5ph3xquz\n"
     ]
    }
   ],
   "source": [
    "sweep_id = wandb.sweep(sweep_config, project=\"Calib_eval_divide_Raw_2\")\n",
    "config = wandb.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mat_file_train(num_subject, chop, x_group, y_group):\n",
    "    # mat_file = io.loadmat('/Users/goldenyoo/Library/Mobile Documents/com~apple~CloudDocs/BioCAS_prepare/Python_code/Data_center/one_dx/Calib_data_'+ str(num_subject) +'.mat')\n",
    "    # mat_file = io.loadmat('C:/Users/Peter/iCloudDrive/BioCAS_prepare/BCIIV_2a_mat/myData/Raw/Calib_data_'+ str(num_subject) +'_chop_'+str(chop) +'.mat')\n",
    "    mat_file = io.loadmat('C:/Users/유승재/iCloudDrive/BioCAS_prepare/BCIIV_2a_mat/myData/Raw/Calib_data_'+ str(num_subject) +'_chop_'+str(chop) +'.mat')\n",
    "    \n",
    "\n",
    "    X1 = mat_file['X1']\n",
    "    X2 = mat_file['X2']\n",
    "\n",
    "    # K2 = mat_file['K2']\n",
    "    # A2 = mat_file['A2']\n",
    "\n",
    "    Y1 = mat_file['Y1']\n",
    "    Y2 = mat_file['Y2']\n",
    "\n",
    "    # K 특성에 대한 Class1 vs Class2 Data 가져오기\n",
    "    x1 = torch.FloatTensor(X1)\n",
    "    x1 = x1.transpose(0,2)\n",
    "\n",
    "    x2 = torch.FloatTensor(X2)\n",
    "    x2 = x2.transpose(0,2)\n",
    "\n",
    "    # Y에 대한 Class1 vs Class2 Data 가져오기\n",
    "    y1 = torch.LongTensor(Y1)\n",
    "    y2 = torch.LongTensor(Y2)\n",
    "\n",
    "    x_train = torch.cat([x1,x2],dim=0)\n",
    "\n",
    "    y_train = torch.cat([y1,y2],dim=0)\n",
    "    y_train = y_train-1 # y를 0~1의 정수로 만들어야함.\n",
    "\n",
    "    #group 만들기\n",
    "    if num_subject == 1:\n",
    "        x_group = x_train.to(device)\n",
    "        y_group = y_train.to(device)\n",
    "    else:\n",
    "        x_group = torch.cat([x_group.to(device),x_train.to(device)], dim=0).to(device)\n",
    "        y_group = torch.cat([y_group.to(device),y_train.to(device)], dim=0).to(device)\n",
    "\n",
    "    return x_group, y_group\n",
    "\n",
    "\n",
    "def load_mat_file_test(num_subject, chop,  x_group, y_group):\n",
    "    # mat_file = io.loadmat('/Users/goldenyoo/Library/Mobile Documents/com~apple~CloudDocs/BioCAS_prepare/Python_code/Data_center/one_dx/Eval_data_'+ str(num_subject) +'.mat')\n",
    "    # mat_file = io.loadmat('C:/Users/Peter/iCloudDrive/BioCAS_prepare/BCIIV_2a_mat/myData/Raw/Eval_data_'+ str(num_subject) +'_chop_'+str(chop) +'.mat')\n",
    "    mat_file = io.loadmat('C:/Users/유승재/iCloudDrive/BioCAS_prepare/BCIIV_2a_mat/myData/Raw/Eval_data_'+ str(num_subject) +'_chop_'+str(chop) +'.mat')\n",
    "\n",
    "    X1 = mat_file['X1']\n",
    "    X2 = mat_file['X2']\n",
    "\n",
    "    Y1 = mat_file['Y1']\n",
    "    Y2 = mat_file['Y2']\n",
    "\n",
    "    # K 특성에 대한 Class1 vs Class2 Data 가져오기\n",
    "    x1 = torch.FloatTensor(X1)\n",
    "    x1 = x1.transpose(0,2)\n",
    "\n",
    "    x2 = torch.FloatTensor(X2)\n",
    "    x2 = x2.transpose(0,2)\n",
    "\n",
    "    # Y에 대한 Class1 vs Class2 Data 가져오기\n",
    "    y1 = torch.LongTensor(Y1)\n",
    "    y2 = torch.LongTensor(Y2)\n",
    "\n",
    "    x_train = torch.cat([x1,x2],dim=0)\n",
    "\n",
    "    y_train = torch.cat([y1,y2],dim=0)\n",
    "    y_train = y_train-1 # y를 0~1의 정수로 만들어야함.\n",
    "\n",
    "    #group 만들기\n",
    "    if num_subject == 0:\n",
    "        x_group = x_train.to(device)\n",
    "        y_group = y_train.to(device)\n",
    "    else:\n",
    "        x_group = torch.cat([x_group.to(device),x_train.to(device)], dim=0)\n",
    "        y_group = torch.cat([y_group.to(device),y_train.to(device)], dim=0)\n",
    "\n",
    "    return x_group, y_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset(batch_size,x_train, y_train, x_test, y_test):\n",
    "    dataset_train = TensorDataset(x_train.to(device),   y_train.to(device)) # 각 tensor의 첫번째 dim이 일치해야한다\n",
    "    dataset_test  = TensorDataset(x_test.to(device) ,   y_test.to(device) ) # 각 tensor의 첫번째 dim이 일치해야한다\n",
    "\n",
    "    # Data Split\n",
    "    dataset_size = len(dataset_train)\n",
    "    train_size = int(dataset_size * 0.9)\n",
    "    validation_size = dataset_size - train_size\n",
    "    train_dataset, valid_dataset = random_split(dataset_train, [train_size, validation_size])\n",
    "\n",
    "    train_dataloader    = DataLoader(train_dataset  ,batch_size=      batch_size  , shuffle=True, drop_last=True)\n",
    "    valid_dataloader    = DataLoader(valid_dataset  ,batch_size= validation_size, shuffle=True, drop_last=True)\n",
    "\n",
    "    # Data Split\n",
    "    test_size = len(dataset_test)\n",
    "    test_dataloader = DataLoader(dataset_test, batch_size= test_size, shuffle=True, drop_last=True)\n",
    "\n",
    "    return train_dataloader, valid_dataloader, test_dataloader,  validation_size,test_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_optimizer(network, optimizer, learning_rate):\n",
    "    if optimizer == \"sgd\":\n",
    "        optimizer = optim.SGD(network.parameters(),\n",
    "                              lr=learning_rate, momentum=0.9)\n",
    "    elif optimizer == \"adam\":\n",
    "        optimizer = optim.Adam(network.parameters(),\n",
    "                               lr=learning_rate)\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 22\n",
    "n_class = 2\n",
    "\n",
    "dtype = torch.float\n",
    "\n",
    "class TextLSTM(nn.Module):\n",
    "  def __init__(self,hidden_size):\n",
    "    super(TextLSTM, self).__init__()\n",
    "\n",
    "    self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size)\n",
    "    self.fc_1 = nn.Linear(hidden_size, hidden_size)\n",
    "    self.fc_2 = nn.Linear(hidden_size, n_class)\n",
    "    \n",
    "\n",
    "  def forward(self, hidden_and_cell, X):\n",
    "\n",
    "    x = X.transpose(1,2)\n",
    "    x = x.transpose(0,1)\n",
    "\n",
    "    outputs1, (h_n1,c_n1) = self.lstm(x, hidden_and_cell)\n",
    "\n",
    "    outputs = outputs1[-1]\n",
    "\n",
    "    model = F.relu(self.fc_1(outputs))  # 최종 예측 최종 출력 층\n",
    "    model = self.fc_2(model)\n",
    "\n",
    "    return model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(config=None):\n",
    "    # Initialize a new wandb run\n",
    "    with wandb.init(config=config):\n",
    "        # If called by wandb.agent, as below,\n",
    "        # this config will be set by Sweep Controller\n",
    "        config = wandb.config\n",
    "\n",
    "\n",
    "        x_train = torch.tensor([]).to(device)\n",
    "        y_train = torch.LongTensor([]).to(device)\n",
    "\n",
    "        x_test = torch.tensor([]).to(device)\n",
    "        y_test = torch.LongTensor([]).to(device)\n",
    "\n",
    "        x_train, y_train = load_mat_file_train(config.subject_label, config.chop,x_train, y_train)\n",
    "        x_test, y_test = load_mat_file_test(config.subject_label, config.chop,x_test,  y_test)\n",
    "\n",
    "        model = TextLSTM(hidden_size=config.hidden_size).to(device)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = build_optimizer(model, config.optimizer, config.learning_rate)\n",
    "        scheduler = optim.lr_scheduler.LambdaLR(optimizer=optimizer,\n",
    "                                        lr_lambda=lambda epoch: 0.95 ** epoch,\n",
    "                                        last_epoch=-1,\n",
    "                                        verbose=False)\n",
    "\n",
    "        model.train()\n",
    "        \n",
    "        train_dataloader, valid_dataloader, test_dataloader,  validation_size,test_size = build_dataset(config.batch_size, x_train.to(device),  y_train.to(device),x_test.to(device),  y_test.to(device))\n",
    "\n",
    "        for epoch in range(config.epochs):\n",
    "            rloss = 0\n",
    "            for batch_idx, samples in enumerate(train_dataloader):\n",
    "\n",
    "                x_train_mb, y_train_mb = samples\n",
    "\n",
    "                hidden  = torch.zeros(1, config.batch_size, config.hidden_size, requires_grad=True).to(device)\n",
    "                cell    = torch.zeros(1, config.batch_size, config.hidden_size, requires_grad=True).to(device)\n",
    "\n",
    "                # Forward\n",
    "                output = model((hidden, cell),  x_train_mb.to(device))\n",
    "\n",
    "                # Cost\n",
    "                loss = criterion(output.to(device), y_train_mb.squeeze().to(device))\n",
    "\n",
    "                # if (epoch) % 100 == 0 and batch_idx % 2 == 0:\n",
    "                #     print('Epoch {:3d}/{} Batch: {:2d} Cost: {:.6f}'.format(epoch, config.epochs, batch_idx, loss))\n",
    "                \n",
    "                # Backpropagate\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                loss_b = loss.item()*config.batch_size\n",
    "                rloss += loss_b\n",
    "            loss_e = rloss/len(train_dataloader.dataset) # epoch loss \n",
    "            wandb.log({\"loss\": loss_e})\n",
    "            if epoch % 100 == 0:\n",
    "                print(f\"Epoch: {epoch}, train loss: {round(loss_e,3)}\")   \n",
    "            scheduler.step()\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, samples in enumerate(test_dataloader):\n",
    "                x_train_mb,  y_train_mb = samples\n",
    "\n",
    "                hidden    = torch.zeros(1, test_size, config.hidden_size).to(device)\n",
    "                cell      = torch.zeros(1, test_size, config.hidden_size).to(device)\n",
    "\n",
    "                output = model((hidden, cell), x_train_mb.to(device))\n",
    "                prediction = output.argmax(dim=1)\n",
    "                correct = prediction.eq(y_train_mb.view_as(prediction)).sum().item()\n",
    "                print(correct/test_size)\n",
    "                wandb.log({\"accuracy\": correct/test_size})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: gkwxdvfq with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tchop: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 500\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: sgd\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsubject_label: 1\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mgoldenyoo\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\유승재\\Desktop\\git_folder\\wandb\\run-20220906_095703-gkwxdvfq</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/goldenyoo/Calib_eval_divide_Raw_2/runs/gkwxdvfq\" target=\"_blank\">crisp-sweep-1</a></strong> to <a href=\"https://wandb.ai/goldenyoo/Calib_eval_divide_Raw_2\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/goldenyoo/Calib_eval_divide_Raw_2/sweeps/5ph3xquz\" target=\"_blank\">https://wandb.ai/goldenyoo/Calib_eval_divide_Raw_2/sweeps/5ph3xquz</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:red\">(failed 1).</strong> Press Ctrl-C to abort syncing."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">crisp-sweep-1</strong>: <a href=\"https://wandb.ai/goldenyoo/Calib_eval_divide_Raw_2/runs/gkwxdvfq\" target=\"_blank\">https://wandb.ai/goldenyoo/Calib_eval_divide_Raw_2/runs/gkwxdvfq</a><br/>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20220906_095703-gkwxdvfq\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Run gkwxdvfq errored: TypeError(\"build_dataset() missing 2 required positional arguments: 'x_test' and 'y_test'\")\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run gkwxdvfq errored: TypeError(\"build_dataset() missing 2 required positional arguments: 'x_test' and 'y_test'\")\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Ctrl + C detected. Stopping sweep.\n"
     ]
    }
   ],
   "source": [
    "wandb.agent(sweep_id, train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('Bus')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e869fb8c73aac89e6a89679ac2355ec9c1d34e4cfb98156c9dcbd0d88abe8f7b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
